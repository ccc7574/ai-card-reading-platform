// RAGå¼•æ“ - åŸºäºæœ€æ–°RAGæŠ€æœ¯çš„æ™ºèƒ½æ•°æ®å¤„ç†ç³»ç»Ÿ
import { OpenAI } from 'openai'
import { agenticRAGEngine, AgenticQuery } from './agentic-rag-engine'
import { GoogleGenerativeAI } from '@google/generative-ai'

export interface VectorDocument {
  id: string
  content: string
  metadata: {
    title: string
    source: string
    category: string
    tags: string[]
    publishedAt: Date
    author?: string
    url?: string
    difficulty?: string
    readingTime?: number
  }
  embedding: number[]
  chunks: DocumentChunk[]
  lastUpdated: Date
}

export interface DocumentChunk {
  id: string
  content: string
  embedding: number[]
  position: number
  metadata: {
    chunkType: 'title' | 'summary' | 'content' | 'conclusion'
    importance: number
    keywords: string[]
  }
}

export interface RAGQuery {
  query: string
  userId?: string
  context?: any
  filters?: {
    category?: string[]
    source?: string[]
    dateRange?: { start: Date; end: Date }
    difficulty?: string[]
    tags?: string[]
  }
  options?: {
    topK?: number
    threshold?: number
    rerank?: boolean
    includeMetadata?: boolean
    mode?: 'traditional' | 'agentic' // æ–°å¢ï¼šæ”¯æŒAgenticæ¨¡å¼
    agenticConfig?: {
      intent?: 'research' | 'learning' | 'problem_solving' | 'exploration'
      complexity?: 'simple' | 'medium' | 'complex'
      maxSteps?: number
      enableReasoning?: boolean
    }
  }
}

export interface RAGResponse {
  results: VectorDocument[]
  query: string
  totalResults: number
  processingTime: number
  confidence: number
  explanation: string
  relatedQueries: string[]
  sources: string[]
  agenticData?: {
    finalAnswer: string
    retrievalSteps: any[]
    agentDecisions: any[]
    qualityScore: number
  }
}

export class RAGEngine {
  private static instance: RAGEngine
  private openai: OpenAI
  private gemini: GoogleGenerativeAI
  private vectorStore = new Map<string, VectorDocument>()
  private queryCache = new Map<string, RAGResponse>()
  private embeddingCache = new Map<string, number[]>()

  private constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    })
    this.gemini = new GoogleGenerativeAI(process.env.GEMINI_API_KEY || '')
  }

  public static getInstance(): RAGEngine {
    if (!RAGEngine.instance) {
      RAGEngine.instance = new RAGEngine()
    }
    return RAGEngine.instance
  }

  // æ™ºèƒ½æ–‡æ¡£å¤„ç†å’Œå‘é‡åŒ–
  public async processDocument(document: any): Promise<VectorDocument> {
    console.log(`ğŸ” RAGå¤„ç†æ–‡æ¡£: ${document.title}`)

    try {
      // 1. æ™ºèƒ½å†…å®¹åˆ†å—
      const chunks = await this.intelligentChunking(document.content, document.title)
      
      // 2. ç”Ÿæˆæ–‡æ¡£çº§åˆ«çš„embedding
      const documentEmbedding = await this.generateEmbedding(
        `${document.title}\n${document.summary}\n${document.content.substring(0, 1000)}`
      )
      
      // 3. ä¸ºæ¯ä¸ªchunkç”Ÿæˆembedding
      const processedChunks: DocumentChunk[] = []
      for (let i = 0; i < chunks.length; i++) {
        const chunk = chunks[i]
        const chunkEmbedding = await this.generateEmbedding(chunk.content)
        
        processedChunks.push({
          id: `${document.id}_chunk_${i}`,
          content: chunk.content,
          embedding: chunkEmbedding,
          position: i,
          metadata: {
            chunkType: chunk.type,
            importance: chunk.importance,
            keywords: await this.extractKeywords(chunk.content)
          }
        })
      }

      // 4. æ™ºèƒ½æ ‡ç­¾æå–å’Œåˆ†ç±»
      const enhancedMetadata = await this.enhanceMetadata(document)

      const vectorDocument: VectorDocument = {
        id: document.id,
        content: document.content,
        metadata: {
          ...document,
          ...enhancedMetadata
        },
        embedding: documentEmbedding,
        chunks: processedChunks,
        lastUpdated: new Date()
      }

      // 5. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
      this.vectorStore.set(document.id, vectorDocument)
      
      console.log(`âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: ${chunks.length} ä¸ªchunks`)
      return vectorDocument

    } catch (error) {
      console.error('RAGæ–‡æ¡£å¤„ç†å¤±è´¥:', error)
      throw error
    }
  }

  // æ™ºèƒ½å†…å®¹åˆ†å—
  private async intelligentChunking(content: string, title: string): Promise<any[]> {
    try {
      const prompt = `
ä½œä¸ºä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œè¯·å°†ä»¥ä¸‹å†…å®¹è¿›è¡Œæ™ºèƒ½åˆ†å—ã€‚
æ¯ä¸ªåˆ†å—åº”è¯¥æ˜¯è¯­ä¹‰å®Œæ•´çš„å•å…ƒï¼ŒåŒ…å«ç›¸å…³çš„æ¦‚å¿µå’Œä¿¡æ¯ã€‚

æ ‡é¢˜: ${title}
å†…å®¹: ${content}

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†å—ç»“æœï¼Œæ¯ä¸ªåˆ†å—åŒ…å«ï¼š
- content: åˆ†å—å†…å®¹
- type: åˆ†å—ç±»å‹ (title/summary/content/conclusion)
- importance: é‡è¦æ€§è¯„åˆ† (1-10)
- summary: åˆ†å—æ‘˜è¦

è¦æ±‚ï¼š
1. æ¯ä¸ªåˆ†å—200-500å­—
2. ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
3. è¯†åˆ«å…³é”®æ¦‚å¿µå’Œè¦ç‚¹
4. æŒ‰é‡è¦æ€§æ’åº
`

      const response = await this.openai.chat.completions.create({
        model: 'gpt-4',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.3
      })

      const result = JSON.parse(response.choices[0].message.content || '[]')
      return Array.isArray(result) ? result : []

    } catch (error) {
      console.error('æ™ºèƒ½åˆ†å—å¤±è´¥:', error)
      // é™çº§åˆ°ç®€å•åˆ†å—
      return this.simpleChunking(content)
    }
  }

  // ç®€å•åˆ†å—ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
  private simpleChunking(content: string): any[] {
    const chunks = []
    const chunkSize = 400
    const overlap = 50

    for (let i = 0; i < content.length; i += chunkSize - overlap) {
      const chunk = content.slice(i, i + chunkSize)
      chunks.push({
        content: chunk,
        type: 'content',
        importance: 5,
        summary: chunk.substring(0, 100) + '...'
      })
    }

    return chunks
  }

  // ç”Ÿæˆembeddingå‘é‡
  private async generateEmbedding(text: string): Promise<number[]> {
    // æ£€æŸ¥ç¼“å­˜
    const cacheKey = this.hashText(text)
    if (this.embeddingCache.has(cacheKey)) {
      return this.embeddingCache.get(cacheKey)!
    }

    try {
      const response = await this.openai.embeddings.create({
        model: 'text-embedding-3-large',
        input: text,
        dimensions: 1536 // ä½¿ç”¨æœ€æ–°çš„é«˜ç»´embedding
      })

      const embedding = response.data[0].embedding
      this.embeddingCache.set(cacheKey, embedding)
      
      return embedding

    } catch (error) {
      console.error('ç”Ÿæˆembeddingå¤±è´¥:', error)
      // è¿”å›éšæœºå‘é‡ä½œä¸ºé™çº§
      return Array.from({ length: 1536 }, () => Math.random() - 0.5)
    }
  }

  // æå–å…³é”®è¯
  private async extractKeywords(text: string): Promise<string[]> {
    try {
      const prompt = `
ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–5-10ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼Œè¿”å›JSONæ•°ç»„æ ¼å¼ï¼š

æ–‡æœ¬: ${text.substring(0, 500)}

è¦æ±‚ï¼š
1. æå–æ ¸å¿ƒæ¦‚å¿µå’ŒæŠ€æœ¯æœ¯è¯­
2. åŒ…å«é¢†åŸŸä¸“ä¸šè¯æ±‡
3. é¿å…å¸¸è§åœç”¨è¯
4. è¿”å›æ ¼å¼: ["å…³é”®è¯1", "å…³é”®è¯2", ...]
`

      const response = await this.openai.chat.completions.create({
        model: 'gpt-4',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.2
      })

      const keywords = JSON.parse(response.choices[0].message.content || '[]')
      return Array.isArray(keywords) ? keywords : []

    } catch (error) {
      console.error('å…³é”®è¯æå–å¤±è´¥:', error)
      return []
    }
  }

  // å¢å¼ºå…ƒæ•°æ®
  private async enhanceMetadata(document: any): Promise<any> {
    try {
      const prompt = `
ä½œä¸ºå†…å®¹åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å¹¶æä¾›å¢å¼ºçš„å…ƒæ•°æ®ï¼š

æ ‡é¢˜: ${document.title}
å†…å®¹: ${document.content.substring(0, 1000)}

è¯·è¿”å›JSONæ ¼å¼çš„å¢å¼ºå…ƒæ•°æ®ï¼š
{
  "difficulty": "beginner|intermediate|advanced|expert",
  "readingTime": é¢„ä¼°é˜…è¯»æ—¶é—´(åˆ†é’Ÿ),
  "category": "æ›´ç²¾ç¡®çš„åˆ†ç±»",
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", ...],
  "topics": ["ä¸»é¢˜1", "ä¸»é¢˜2", ...],
  "sentiment": "positive|neutral|negative",
  "complexity": 1-10,
  "actionable": true/false,
  "prerequisites": ["å‰ç½®çŸ¥è¯†1", "å‰ç½®çŸ¥è¯†2", ...],
  "relatedConcepts": ["ç›¸å…³æ¦‚å¿µ1", "ç›¸å…³æ¦‚å¿µ2", ...]
}
`

      const response = await this.openai.chat.completions.create({
        model: 'gpt-4',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.3
      })

      return JSON.parse(response.choices[0].message.content || '{}')

    } catch (error) {
      console.error('å…ƒæ•°æ®å¢å¼ºå¤±è´¥:', error)
      return {}
    }
  }

  // æ™ºèƒ½æ£€ç´¢
  public async search(query: RAGQuery): Promise<RAGResponse> {
    const startTime = Date.now()

    try {
      console.log(`ğŸ” RAGæ£€ç´¢ (${query.options?.mode || 'traditional'}): ${query.query}`)

      // æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Agenticæ¨¡å¼
      if (query.options?.mode === 'agentic') {
        return await this.agenticSearch(query)
      }

      // 1. æ£€æŸ¥æŸ¥è¯¢ç¼“å­˜
      const cacheKey = this.hashQuery(query)
      if (this.queryCache.has(cacheKey)) {
        const cached = this.queryCache.get(cacheKey)!
        console.log('âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ')
        return cached
      }

      // 2. æŸ¥è¯¢æ‰©å±•å’Œé‡å†™
      const expandedQuery = await this.expandQuery(query.query, query.context)

      // 3. ç”ŸæˆæŸ¥è¯¢embedding
      const queryEmbedding = await this.generateEmbedding(expandedQuery)

      // 4. å‘é‡ç›¸ä¼¼åº¦æœç´¢
      const candidates = await this.vectorSearch(queryEmbedding, query.filters, query.options?.topK || 10)

      // 5. é‡æ’åº
      const rerankedResults = query.options?.rerank 
        ? await this.rerank(candidates, query.query)
        : candidates

      // 6. ç”Ÿæˆè§£é‡Šå’Œç›¸å…³æŸ¥è¯¢
      const explanation = await this.generateExplanation(query.query, rerankedResults)
      const relatedQueries = await this.generateRelatedQueries(query.query)

      const response: RAGResponse = {
        results: rerankedResults.slice(0, query.options?.topK || 10),
        query: query.query,
        totalResults: candidates.length,
        processingTime: Date.now() - startTime,
        confidence: this.calculateConfidence(rerankedResults),
        explanation,
        relatedQueries,
        sources: [...new Set(rerankedResults.map(r => r.metadata.source))]
      }

      // 7. ç¼“å­˜ç»“æœ
      this.queryCache.set(cacheKey, response)

      console.log(`âœ… RAGæ£€ç´¢å®Œæˆ: ${response.results.length} ä¸ªç»“æœ`)
      return response

    } catch (error) {
      console.error('RAGæ£€ç´¢å¤±è´¥:', error)
      throw error
    }
  }

  // Agentic RAGæœç´¢
  private async agenticSearch(query: RAGQuery): Promise<RAGResponse> {
    const startTime = Date.now()

    try {
      console.log(`ğŸ§  å¯åŠ¨Agentic RAGæ¨¡å¼`)

      // æ„å»ºAgenticæŸ¥è¯¢
      const agenticQuery: AgenticQuery = {
        originalQuery: query.query,
        userId: query.userId,
        context: {
          userIntent: query.options?.agenticConfig?.intent || 'research',
          complexity: query.options?.agenticConfig?.complexity || 'medium',
          conversationHistory: query.context?.history || []
        },
        constraints: {
          maxSteps: query.options?.agenticConfig?.maxSteps || 5,
          timeLimit: 30000,
          language: 'zh'
        }
      }

      // è°ƒç”¨Agentic RAGå¼•æ“
      const agenticResult = await agenticRAGEngine.processQuery(agenticQuery)

      // è½¬æ¢ä¸ºRAGå“åº”æ ¼å¼
      const ragResponse: RAGResponse = {
        results: [], // Agentic RAGæœ‰è‡ªå·±çš„ç»“æœæ ¼å¼
        query: query.query,
        totalResults: agenticResult.sources.length,
        processingTime: Date.now() - startTime,
        confidence: agenticResult.confidence,
        explanation: agenticResult.reasoning,
        relatedQueries: [], // å¯ä»¥ä»Agenticç»“æœä¸­æå–
        sources: agenticResult.sources.map(s => s.title),
        // æ·»åŠ Agenticç‰¹æœ‰çš„ä¿¡æ¯
        agenticData: {
          finalAnswer: agenticResult.finalAnswer,
          retrievalSteps: agenticResult.retrievalSteps,
          agentDecisions: agenticResult.metadata.agentDecisions,
          qualityScore: agenticResult.metadata.qualityScore
        }
      }

      console.log(`âœ… Agentic RAGå®Œæˆ: ${agenticResult.metadata.totalSteps} æ­¥éª¤`)
      return ragResponse

    } catch (error) {
      console.error('Agentic RAGæœç´¢å¤±è´¥:', error)

      // é™çº§åˆ°ä¼ ç»ŸRAG
      console.log('ğŸ”„ é™çº§åˆ°ä¼ ç»ŸRAGæ¨¡å¼')
      const fallbackQuery: RAGQuery = {
        ...query,
        options: {
          ...query.options,
          mode: 'traditional' as const
        }
      }
      return await this.search(fallbackQuery)
    }
  }

  // æŸ¥è¯¢æ‰©å±•
  private async expandQuery(query: string, context?: any): Promise<string> {
    try {
      const prompt = `
ä½œä¸ºæŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ï¼Œè¯·æ‰©å±•å’Œä¼˜åŒ–ä»¥ä¸‹æŸ¥è¯¢ï¼Œä½¿å…¶æ›´é€‚åˆè¯­ä¹‰æœç´¢ï¼š

åŸå§‹æŸ¥è¯¢: ${query}
ä¸Šä¸‹æ–‡: ${context ? JSON.stringify(context) : 'æ— '}

è¯·è¿”å›ä¼˜åŒ–åçš„æŸ¥è¯¢ï¼Œè¦æ±‚ï¼š
1. ä¿æŒåŸæ„
2. æ·»åŠ ç›¸å…³åŒä¹‰è¯
3. åŒ…å«é¢†åŸŸæœ¯è¯­
4. é€‚åˆå‘é‡æ£€ç´¢

åªè¿”å›ä¼˜åŒ–åçš„æŸ¥è¯¢æ–‡æœ¬ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
`

      const response = await this.openai.chat.completions.create({
        model: 'gpt-4',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.3
      })

      return response.choices[0].message.content || query

    } catch (error) {
      console.error('æŸ¥è¯¢æ‰©å±•å¤±è´¥:', error)
      return query
    }
  }

  // å‘é‡æœç´¢
  private async vectorSearch(
    queryEmbedding: number[], 
    filters?: RAGQuery['filters'], 
    topK: number = 10
  ): Promise<VectorDocument[]> {
    const candidates: { document: VectorDocument; score: number }[] = []

    for (const [id, document] of this.vectorStore) {
      // åº”ç”¨è¿‡æ»¤å™¨
      if (filters && !this.applyFilters(document, filters)) {
        continue
      }

      // è®¡ç®—ç›¸ä¼¼åº¦
      const similarity = this.cosineSimilarity(queryEmbedding, document.embedding)
      
      // åŒæ—¶æ£€æŸ¥chunkçº§åˆ«çš„ç›¸ä¼¼åº¦
      let maxChunkSimilarity = 0
      for (const chunk of document.chunks) {
        const chunkSimilarity = this.cosineSimilarity(queryEmbedding, chunk.embedding)
        maxChunkSimilarity = Math.max(maxChunkSimilarity, chunkSimilarity)
      }

      // ç»¼åˆæ–‡æ¡£å’Œchunkç›¸ä¼¼åº¦
      const finalScore = similarity * 0.7 + maxChunkSimilarity * 0.3

      candidates.push({ document, score: finalScore })
    }

    // æŒ‰ç›¸ä¼¼åº¦æ’åº
    candidates.sort((a, b) => b.score - a.score)

    return candidates.slice(0, topK * 2).map(c => c.document) // è¿”å›æ›´å¤šå€™é€‰ç”¨äºé‡æ’åº
  }

  // é‡æ’åº
  private async rerank(documents: VectorDocument[], query: string): Promise<VectorDocument[]> {
    try {
      const prompt = `
ä½œä¸ºå†…å®¹ç›¸å…³æ€§ä¸“å®¶ï¼Œè¯·æ ¹æ®æŸ¥è¯¢å¯¹ä»¥ä¸‹æ–‡æ¡£è¿›è¡Œé‡æ’åºï¼š

æŸ¥è¯¢: ${query}

æ–‡æ¡£åˆ—è¡¨:
${documents.map((doc, i) => `${i + 1}. ${doc.metadata.title}\næ‘˜è¦: ${doc.content.substring(0, 200)}...`).join('\n\n')}

è¯·è¿”å›é‡æ’åºåçš„æ–‡æ¡£IDåˆ—è¡¨ï¼Œæ ¼å¼ä¸ºJSONæ•°ç»„: ["id1", "id2", ...]
æŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½æ’åºã€‚
`

      const response = await this.openai.chat.completions.create({
        model: 'gpt-4',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.2
      })

      const reorderedIds = JSON.parse(response.choices[0].message.content || '[]')
      
      // æ ¹æ®é‡æ’åºç»“æœé‡æ–°ç»„ç»‡æ–‡æ¡£
      const reranked: VectorDocument[] = []
      for (const id of reorderedIds) {
        const doc = documents.find(d => d.id === id)
        if (doc) reranked.push(doc)
      }

      // æ·»åŠ æœªåœ¨é‡æ’åºä¸­çš„æ–‡æ¡£
      for (const doc of documents) {
        if (!reranked.find(r => r.id === doc.id)) {
          reranked.push(doc)
        }
      }

      return reranked

    } catch (error) {
      console.error('é‡æ’åºå¤±è´¥:', error)
      return documents
    }
  }

  // ç”Ÿæˆè§£é‡Š
  private async generateExplanation(query: string, results: VectorDocument[]): Promise<string> {
    try {
      const prompt = `
è¯·ä¸ºä»¥ä¸‹æ£€ç´¢ç»“æœç”Ÿæˆç®€æ´çš„è§£é‡Šï¼š

æŸ¥è¯¢: ${query}
ç»“æœæ•°é‡: ${results.length}
ä¸»è¦æ¥æº: ${[...new Set(results.slice(0, 3).map(r => r.metadata.source))].join(', ')}

è¯·ç”Ÿæˆä¸€å¥è¯çš„è§£é‡Šï¼Œè¯´æ˜ä¸ºä»€ä¹ˆè¿™äº›ç»“æœä¸æŸ¥è¯¢ç›¸å…³ã€‚
`

      const response = await this.openai.chat.completions.create({
        model: 'gpt-4',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.3
      })

      return response.choices[0].message.content || 'åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…çš„ç›¸å…³å†…å®¹'

    } catch (error) {
      console.error('ç”Ÿæˆè§£é‡Šå¤±è´¥:', error)
      return 'åŸºäºAIè¯­ä¹‰ç†è§£çš„æ™ºèƒ½æ£€ç´¢ç»“æœ'
    }
  }

  // ç”Ÿæˆç›¸å…³æŸ¥è¯¢
  private async generateRelatedQueries(query: string): Promise<string[]> {
    try {
      const prompt = `
åŸºäºæŸ¥è¯¢"${query}"ï¼Œç”Ÿæˆ3-5ä¸ªç›¸å…³çš„æŸ¥è¯¢å»ºè®®ã€‚
è¿”å›JSONæ•°ç»„æ ¼å¼: ["æŸ¥è¯¢1", "æŸ¥è¯¢2", ...]

è¦æ±‚ï¼š
1. ä¸åŸæŸ¥è¯¢ç›¸å…³ä½†è§’åº¦ä¸åŒ
2. å¯èƒ½æ˜¯ç”¨æˆ·çš„åç»­é—®é¢˜
3. æ¶µç›–ç›¸å…³ä¸»é¢˜
`

      const response = await this.openai.chat.completions.create({
        model: 'gpt-4',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.5
      })

      const related = JSON.parse(response.choices[0].message.content || '[]')
      return Array.isArray(related) ? related : []

    } catch (error) {
      console.error('ç”Ÿæˆç›¸å…³æŸ¥è¯¢å¤±è´¥:', error)
      return []
    }
  }

  // åº”ç”¨è¿‡æ»¤å™¨
  private applyFilters(document: VectorDocument, filters: RAGQuery['filters']): boolean {
    if (filters?.category && !filters.category.includes(document.metadata.category)) {
      return false
    }

    if (filters?.source && !filters.source.includes(document.metadata.source)) {
      return false
    }

    if (filters?.difficulty && !filters.difficulty.includes(document.metadata.difficulty || '')) {
      return false
    }

    if (filters?.tags && !filters.tags.some(tag => document.metadata.tags.includes(tag))) {
      return false
    }

    if (filters?.dateRange) {
      const docDate = new Date(document.metadata.publishedAt)
      if (docDate < filters.dateRange.start || docDate > filters.dateRange.end) {
        return false
      }
    }

    return true
  }

  // è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
  private cosineSimilarity(a: number[], b: number[]): number {
    if (a.length !== b.length) return 0

    let dotProduct = 0
    let normA = 0
    let normB = 0

    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i]
      normA += a[i] * a[i]
      normB += b[i] * b[i]
    }

    const magnitude = Math.sqrt(normA) * Math.sqrt(normB)
    return magnitude === 0 ? 0 : dotProduct / magnitude
  }

  // è®¡ç®—ç½®ä¿¡åº¦
  private calculateConfidence(results: VectorDocument[]): number {
    if (results.length === 0) return 0
    
    // åŸºäºç»“æœæ•°é‡å’Œè´¨é‡è®¡ç®—ç½®ä¿¡åº¦
    const baseConfidence = Math.min(results.length / 10, 1) * 0.5
    const qualityBonus = results.length > 0 ? 0.3 : 0
    const diversityBonus = new Set(results.map(r => r.metadata.source)).size > 1 ? 0.2 : 0
    
    return Math.min(baseConfidence + qualityBonus + diversityBonus, 1)
  }

  // å“ˆå¸Œå‡½æ•°
  private hashText(text: string): string {
    let hash = 0
    for (let i = 0; i < text.length; i++) {
      const char = text.charCodeAt(i)
      hash = ((hash << 5) - hash) + char
      hash = hash & hash // è½¬æ¢ä¸º32ä½æ•´æ•°
    }
    return hash.toString()
  }

  private hashQuery(query: RAGQuery): string {
    return this.hashText(JSON.stringify({
      query: query.query,
      filters: query.filters,
      options: query.options
    }))
  }

  // è·å–ç»Ÿè®¡ä¿¡æ¯
  public getStats() {
    const totalDocuments = this.vectorStore.size
    const totalChunks = Array.from(this.vectorStore.values())
      .reduce((sum, doc) => sum + doc.chunks.length, 0)
    
    return {
      totalDocuments,
      totalChunks,
      averageChunksPerDocument: totalDocuments > 0 ? totalChunks / totalDocuments : 0,
      cacheSize: {
        queries: this.queryCache.size,
        embeddings: this.embeddingCache.size
      },
      lastUpdated: new Date().toISOString()
    }
  }

  // æ¸…ç†ç¼“å­˜
  public clearCache() {
    this.queryCache.clear()
    this.embeddingCache.clear()
    console.log('âœ… RAGç¼“å­˜å·²æ¸…ç†')
  }
}

// å¯¼å‡ºå•ä¾‹å®ä¾‹
export const ragEngine = RAGEngine.getInstance()
