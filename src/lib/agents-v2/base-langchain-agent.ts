import { BaseMessage, HumanMessage, AIMessage, SystemMessage } from '@langchain/core/messages'
import { ChatOpenAI } from '@langchain/openai'
import { ChatGoogleGenerativeAI } from '@langchain/google-genai'
import { BaseChatModel } from '@langchain/core/language_models/chat_models'
import { StructuredTool } from '@langchain/core/tools'
import { AgentExecutor, createOpenAIFunctionsAgent } from 'langchain/agents'
import { ChatPromptTemplate, MessagesPlaceholder } from '@langchain/core/prompts'
import { nanoid } from 'nanoid'

// Agenté…ç½®æ¥å£
export interface LangChainAgentConfig {
  id?: string
  name: string
  description: string
  systemPrompt: string
  model: 'gpt-4' | 'gpt-3.5-turbo' | 'gemini-pro'
  temperature?: number
  maxTokens?: number
  tools?: StructuredTool[]
  memory?: boolean
}

// Agentæ‰§è¡Œç»“æœ
export interface AgentExecutionResult {
  success: boolean
  output: string
  intermediateSteps?: any[]
  error?: string
  metadata?: {
    tokensUsed?: number
    executionTime?: number
    toolCalls?: string[]
  }
}

// åŸºäºLangChainçš„ç°ä»£AgentåŸºç±»
export class LangChainAgent {
  private config: LangChainAgentConfig
  private llm: BaseChatModel
  private executor: AgentExecutor | null = null
  private conversationHistory: BaseMessage[] = []

  constructor(config: LangChainAgentConfig) {
    this.config = {
      id: config.id || nanoid(),
      temperature: 0.7,
      maxTokens: 2048,
      memory: true,
      ...config
    }

    // åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
    this.llm = this.initializeLLM()
    
    // åˆå§‹åŒ–Agentæ‰§è¡Œå™¨
    this.initializeAgent()
  }

  private initializeLLM(): BaseChatModel {
    switch (this.config.model) {
      case 'gpt-4':
      case 'gpt-3.5-turbo':
        if (!process.env.OPENAI_API_KEY) {
          throw new Error('OPENAI_API_KEY is required for OpenAI models')
        }
        return new ChatOpenAI({
          modelName: this.config.model,
          temperature: this.config.temperature,
          maxTokens: this.config.maxTokens,
          openAIApiKey: process.env.OPENAI_API_KEY,
        })

      case 'gemini-pro':
        if (!process.env.GOOGLE_API_KEY) {
          throw new Error('GOOGLE_API_KEY is required for Gemini models')
        }
        return new ChatGoogleGenerativeAI({
          modelName: 'gemini-pro',
          temperature: this.config.temperature,
          maxOutputTokens: this.config.maxTokens,
          apiKey: process.env.GOOGLE_API_KEY,
        })

      default:
        throw new Error(`Unsupported model: ${this.config.model}`)
    }
  }

  private async initializeAgent() {
    try {
      // åˆ›å»ºæç¤ºæ¨¡æ¿
      const prompt = ChatPromptTemplate.fromMessages([
        ['system', this.config.systemPrompt],
        new MessagesPlaceholder('chat_history'),
        ['human', '{input}'],
        new MessagesPlaceholder('agent_scratchpad'),
      ])

      // åˆ›å»ºAgent
      const agent = await createOpenAIFunctionsAgent({
        llm: this.llm,
        tools: this.config.tools || [],
        prompt,
      })

      // åˆ›å»ºæ‰§è¡Œå™¨
      this.executor = new AgentExecutor({
        agent,
        tools: this.config.tools || [],
        verbose: process.env.NODE_ENV === 'development',
        maxIterations: 5,
        returnIntermediateSteps: true,
      })

      console.log(`ğŸ¤– LangChain Agent "${this.config.name}" åˆå§‹åŒ–å®Œæˆ`)
    } catch (error) {
      console.error(`âŒ Agentåˆå§‹åŒ–å¤±è´¥:`, error)
      throw error
    }
  }

  // æ‰§è¡ŒAgentä»»åŠ¡
  async execute(input: string, context?: Record<string, any>): Promise<AgentExecutionResult> {
    if (!this.executor) {
      throw new Error('Agent executor not initialized')
    }

    const startTime = Date.now()

    try {
      console.log(`ğŸš€ Agent "${this.config.name}" å¼€å§‹æ‰§è¡Œ: ${input.slice(0, 100)}...`)

      // å‡†å¤‡è¾“å…¥
      const agentInput = {
        input,
        chat_history: this.config.memory ? this.conversationHistory : [],
        ...context
      }

      // æ‰§è¡ŒAgent
      const result = await this.executor.invoke(agentInput)

      // æ›´æ–°å¯¹è¯å†å²
      if (this.config.memory) {
        this.conversationHistory.push(new HumanMessage(input))
        this.conversationHistory.push(new AIMessage(result.output))
        
        // é™åˆ¶å†å²é•¿åº¦
        if (this.conversationHistory.length > 20) {
          this.conversationHistory = this.conversationHistory.slice(-20)
        }
      }

      const executionTime = Date.now() - startTime
      console.log(`âœ… Agent "${this.config.name}" æ‰§è¡Œå®Œæˆ (${executionTime}ms)`)

      return {
        success: true,
        output: result.output,
        intermediateSteps: result.intermediateSteps,
        metadata: {
          executionTime,
          toolCalls: result.intermediateSteps?.map((step: any) => step.action?.tool) || []
        }
      }

    } catch (error) {
      const executionTime = Date.now() - startTime
      console.error(`âŒ Agent "${this.config.name}" æ‰§è¡Œå¤±è´¥:`, error)

      return {
        success: false,
        output: '',
        error: error instanceof Error ? error.message : 'Unknown error',
        metadata: {
          executionTime
        }
      }
    }
  }

  // æµå¼æ‰§è¡Œï¼ˆå®æ—¶å“åº”ï¼‰
  async *executeStream(input: string, context?: Record<string, any>): AsyncGenerator<string, void, unknown> {
    if (!this.executor) {
      throw new Error('Agent executor not initialized')
    }

    try {
      const agentInput = {
        input,
        chat_history: this.config.memory ? this.conversationHistory : [],
        ...context
      }

      // ä½¿ç”¨æµå¼æ‰§è¡Œ
      const stream = await this.executor.stream(agentInput)
      
      for await (const chunk of stream) {
        if (chunk.agent?.messages) {
          for (const message of chunk.agent.messages) {
            if (message.content) {
              yield message.content
            }
          }
        }
        
        if (chunk.tools?.messages) {
          for (const message of chunk.tools.messages) {
            yield `ğŸ”§ Tool: ${message.content}\n`
          }
        }
      }

    } catch (error) {
      yield `âŒ Error: ${error instanceof Error ? error.message : 'Unknown error'}`
    }
  }

  // æ·»åŠ å·¥å…·
  addTool(tool: StructuredTool) {
    if (!this.config.tools) {
      this.config.tools = []
    }
    this.config.tools.push(tool)
    
    // é‡æ–°åˆå§‹åŒ–Agentä»¥åŒ…å«æ–°å·¥å…·
    this.initializeAgent()
  }

  // è·å–Agentä¿¡æ¯
  getInfo() {
    return {
      id: this.config.id,
      name: this.config.name,
      description: this.config.description,
      model: this.config.model,
      toolCount: this.config.tools?.length || 0,
      hasMemory: this.config.memory,
      conversationLength: this.conversationHistory.length
    }
  }

  // æ¸…é™¤å¯¹è¯å†å²
  clearHistory() {
    this.conversationHistory = []
  }

  // è·å–å¯¹è¯å†å²
  getHistory(): BaseMessage[] {
    return [...this.conversationHistory]
  }

  // æ›´æ–°ç³»ç»Ÿæç¤º
  updateSystemPrompt(newPrompt: string) {
    this.config.systemPrompt = newPrompt
    this.initializeAgent()
  }

  // åˆ‡æ¢æ¨¡å‹
  async switchModel(newModel: LangChainAgentConfig['model']) {
    this.config.model = newModel
    this.llm = this.initializeLLM()
    await this.initializeAgent()
  }
}
